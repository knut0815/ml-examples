{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Char-RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kylemcdonald/ml-examples/blob/master/workshop/char_rnn/Char-RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YlRWEmD4YCYl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Char RNN\n",
        "\n",
        "This is a short introduction to char-rnn as introduced by Andrej Karpathy in his [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
      ]
    },
    {
      "metadata": {
        "id": "UNNy8eKUYTgQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# next step: move to tensorflow char-rnn\n",
        "# !git clone https://github.com/sherjilozair/char-rnn-tensorflow.git\n",
        "# !python char-rnn-tensorflow/train.py --data_dir=char-rnn-tensorflow/data/tinyshakespeare/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Obx8Faq5YCYn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lfs=require 'lfs'\n",
        "lfs.chdir(\"/root/shared/ml-examples/karpathy/char-rnn\")\n",
        "data_root = \"/root/shared/ml-examples/karpathy/char-rnn/data/tinyshakespeare\"\n",
        "pretrained_root = \"/root/shared/ml-examples/models/char-rnn\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "46mEgWZ_YCYr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sampling\n",
        "\n",
        "First, let's generate text in a similar style to Shakespeare from a pretrained model.\n",
        "\n",
        "You can change any of the following parameters, try various combinations. Karpathy has suggestions for the different parameters [here](https://github.com/karpathy/char-rnn#approximate-number-of-parameters).\n",
        "\n",
        "```\n",
        "Options\n",
        "  <model>      model checkpoint to use for sampling\n",
        "  -seed        random number generator's seed [123]\n",
        "  -sample       0 to use max at each timestep, 1 to sample at each timestep [1]\n",
        "  -primetext   used as a prompt to \"seed\" the state of the LSTM using a given sequence, before we sample. []\n",
        "  -length      number of characters to sample [2000]\n",
        "  -temperature temperature of sampling [1]\n",
        "  -gpuid       which gpu to use. -1 = use CPU [0]\n",
        "  -opencl      use OpenCL (instead of CUDA) [0]\n",
        "  -verbose     set to 0 to ONLY print the sampled text, no diagnostics [1]\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "QfW0uxNHYCYr",
        "colab_type": "code",
        "colab": {},
        "outputId": "682f1311-c393-4895-8a7a-8e6b59da8337"
      },
      "cell_type": "code",
      "source": [
        "model = pretrained_root..\"/tinyshakespeare.t7\"\n",
        "seed = 123\n",
        "sample = 1\n",
        "prime_text = \"the meaning of life is \"\n",
        "length = 500\n",
        "temperature = 1\n",
        "\n",
        "os.execute(\n",
        "    \"th sample.lua \"..model..\n",
        "    \" -seed \"..seed..\n",
        "    \" -sample \"..sample..\n",
        "    \" -primetext \\\"\"..prime_text..\"\\\"\"..\n",
        "    \" -length \"..length..\n",
        "    \" -temperature \"..temperature..\n",
        "    \" -gpuid -1\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "creating an lstm...\t\n",
              "seeding with the meaning of life is \t\n",
              "--------------------------\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "the meaning of life is ready,\n",
              "The sweet bears, be so. Peace that thou hast had fusthes\n",
              "Unsaid with which becomes a liver to know\n",
              "Than heaven her mourney and gentle my lost\n",
              "Of orcagor, for flesh she hath sorchant'd.\n",
              "\n",
              "GLOUCESTER:\n",
              "Fater dooply gone! Sir Kate, in twoshed langus.\n",
              "I, that we must: I kill you allease,\n",
              "Thou hast done thy gain, like deposed.\n",
              "But fray thou not this blessed?\n",
              "\n",
              "COMINIUS:\n",
              "'Tis none, both Grumio:\n",
              "I craight you, take our person.\n",
              "\n",
              "BISHOP OF GABnn:\n",
              "Why, then he dead, 'tis a gettingabitors\n",
              "And my head o\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "JH0wGiveYCYx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "CharRNN outputs the probability what character comes next at each iteration.\n",
        "When `-sample` is set to 1, it samples from probability distribution which CharRNN outputs.\n",
        "\n",
        "Otherwise, if `-sample` is set to 0, it tends to generate such repeated text as you can see:"
      ]
    },
    {
      "metadata": {
        "id": "ugJ1dWtfYCYy",
        "colab_type": "code",
        "colab": {},
        "outputId": "2eb23a76-c811-44a6-f7f0-ed825506ff85"
      },
      "cell_type": "code",
      "source": [
        "sample = 0\n",
        "\n",
        "os.execute(\n",
        "    \"th sample.lua \"..model..\n",
        "    \" -seed \"..seed..\n",
        "    \" -sample \"..sample..\n",
        "    \" -primetext \\\"\"..prime_text..\"\\\"\"..\n",
        "    \" -length \"..length..\n",
        "    \" -temperature \"..temperature..\n",
        "    \" -gpuid -1\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "creating an lstm...\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "seeding with the meaning of life is \t\n",
              "--------------------------\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "the meaning of life is strange,\n",
              "And there the greatest conduct with thee\n",
              "That with the wearest of the senate of the world,\n",
              "That the more state of the house of the common made\n",
              "That she shall stay the sea will be denied,\n",
              "And the more sire of the princess of the world\n",
              "Art thou no more still the sun of me,\n",
              "That we will be a prison.\n",
              "\n",
              "CAMILLO:\n",
              "The sail, of this the senate of the world,\n",
              "That the marriage of the senate of the world,\n",
              "That the seat of the wearing of the head,\n",
              "And then the senate of the strength and sweet,\n",
              "And t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "id": "Z8iyln5yYCY1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "You can also train your own data.\n",
        "If you would like to use your own data, then create a single file input.txt and place it into a folder such as `somefolder/input.txt` for example. \n",
        "\n",
        "Then you should specify: \n",
        "\n",
        "`-data_dir somefolder`\n",
        "\n",
        "\n",
        "Following parameters are changable, minimum parameters you should specify are data_dir and checkpoint_dir.\n",
        "\n",
        "```\n",
        "Options\n",
        "  -data_dir                  data directory. Should contain the file input.txt with input data [data/tinyshakespeare]\n",
        "  -rnn_size                  size of LSTM internal state [128]\n",
        "  -num_layers                number of layers in the LSTM [2]\n",
        "  -model                     lstm,gru or rnn [lstm]\n",
        "  -learning_rate             learning rate [0.002]\n",
        "  -learning_rate_decay       learning rate decay [0.97]\n",
        "  -learning_rate_decay_after in number of epochs, when to start decaying the learning rate [10]\n",
        "  -decay_rate                decay rate for rmsprop [0.95]\n",
        "  -dropout                   dropout for regularization, used after each RNN hidden layer. 0 = no dropout [0]\n",
        "  -seq_length                number of timesteps to unroll for [50]\n",
        "  -batch_size                number of sequences to train on in parallel [50]\n",
        "  -max_epochs                number of full passes through the training data [50]\n",
        "  -grad_clip                 clip gradients at this value [5]\n",
        "  -train_frac                fraction of data that goes into train set [0.95]\n",
        "  -val_frac                  fraction of data that goes into validation set [0.05]\n",
        "  -init_from                 initialize network parameters from checkpoint at this path []\n",
        "  -seed                      torch manual random number generator seed [123]\n",
        "  -print_every               how many steps/minibatches between printing out the loss [1]\n",
        "  -eval_val_every            every how many iterations should we evaluate on validation data? [1000]\n",
        "  -checkpoint_dir            output directory where checkpoints get written [cv]\n",
        "  -savefile                  filename to autosave the checkpont to. Will be inside checkpoint_dir/ [lstm]\n",
        "  -accurate_gpu_timing       set this flag to 1 to get precise timings when using GPU. Might make code bit slower but reports accurate timings. [0]\n",
        "  -gpuid                     which gpu to use. -1 = use CPU [0]\n",
        "  -opencl                    use OpenCL (instead of CUDA) [0]\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "x6PidMNZYCY2",
        "colab_type": "code",
        "colab": {},
        "outputId": "1da5e308-3815-4277-a124-1606866968c0"
      },
      "cell_type": "code",
      "source": [
        "data_dir = \"/root/shared/ml-examples/karpathy/char-rnn/data/tinyshakespeare\"\n",
        "os.execute(\n",
        "    \"th train.lua \"..\n",
        "    \" -data_dir \\\"\"..data_dir..\"\\\"\"..\n",
        "    \" -checkpoint_dir \\\"\"..data_dir..\"\\\"\"..\n",
        "    \" -gpuid -1\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "loading data files...\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "cutting off end of data so that the batches/sequences divide evenly\t\n",
              "reshaping tensor...\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "data load done. Number of data batches in train: 423, val: 23, test: 0\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "vocab size: 65\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "creating an lstm with 2 layers\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "setting forget gate biases to 1 in LSTM layer 1\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "setting forget gate biases to 1 in LSTM layer 2\t\n",
              "number of parameters in the model: 240321\t\n",
              "cloning rnn\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "cloning criterion\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1/21150 (epoch 0.002), train_loss = 4.19803724, grad/param norm = 5.1721e-01, time/batch = 0.4729s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2/21150 (epoch 0.005), train_loss = 3.93712133, grad/param norm = 1.4679e+00, time/batch = 0.5227s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3/21150 (epoch 0.007), train_loss = 3.43764434, grad/param norm = 9.5800e-01, time/batch = 0.4477s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4/21150 (epoch 0.009), train_loss = 3.41313742, grad/param norm = 7.5143e-01, time/batch = 0.4013s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5/21150 (epoch 0.012), train_loss = 3.33707270, grad/param norm = 6.9269e-01, time/batch = 0.3920s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6/21150 (epoch 0.014), train_loss = 3.37127145, grad/param norm = 5.2318e-01, time/batch = 0.3949s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7/21150 (epoch 0.017), train_loss = 3.36724018, grad/param norm = 4.3217e-01, time/batch = 0.4494s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8/21150 (epoch 0.019), train_loss = 3.33067083, grad/param norm = 3.9964e-01, time/batch = 0.4644s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9/21150 (epoch 0.021), train_loss = 3.29356131, grad/param norm = 3.8693e-01, time/batch = 0.4116s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10/21150 (epoch 0.024), train_loss = 3.38283139, grad/param norm = 3.5561e-01, time/batch = 0.4068s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11/21150 (epoch 0.026), train_loss = 3.30195265, grad/param norm = 3.5806e-01, time/batch = 0.4057s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12/21150 (epoch 0.028), train_loss = 3.32249605, grad/param norm = 2.7507e-01, time/batch = 0.3873s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13/21150 (epoch 0.031), train_loss = 3.30913857, grad/param norm = 2.4440e-01, time/batch = 0.4009s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14/21150 (epoch 0.033), train_loss = 3.28707813, grad/param norm = 3.4650e-01, time/batch = 0.4010s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15/21150 (epoch 0.035), train_loss = 3.36023106, grad/param norm = 3.9640e-01, time/batch = 0.3969s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16/21150 (epoch 0.038), train_loss = 3.33863527, grad/param norm = 3.4813e-01, time/batch = 0.3973s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17/21150 (epoch 0.040), train_loss = 3.29905544, grad/param norm = 3.9863e-01, time/batch = 0.4140s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18/21150 (epoch 0.043), train_loss = 3.31918335, grad/param norm = 2.5565e-01, time/batch = 0.4016s\t\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    }
  ]
}